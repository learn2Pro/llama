{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learn2Pro/llama/blob/main/learnllama/lora_inject_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkzrQ7ayOC-m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fCFD7flbPKW8",
        "outputId": "e8facfdd-0846-482b-ffe0-8aa21fab0e8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Downloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- references\n",
        "  - https://huggingface.co/docs/peft/developer_guides/low_level_api"
      ],
      "metadata": {
        "id": "-juuig3EOekE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9hbSRDG7OC-o"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import torch\n",
        "from peft import inject_adapter_in_model, LoraConfig\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(100, 1000)\n",
        "    self.linear = nn.Linear(1000, 1000)\n",
        "    self.lm_head = nn.Linear(1000, 100)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.linear(x)\n",
        "    x = self.lm_head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9XeraEUFOjCr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"linear\"],\n",
        ")\n",
        "lora_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_S0a0d1PcCa",
        "outputId": "bdfef10f-8d5e-4a9c-c4d5-3be103e9a429"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=64, target_modules={'linear'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyModel()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvE57_m1P1U8",
        "outputId": "142666d3-60ec-44b6-9c88-5da1333f533c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DummyModel(\n",
              "  (embedding): Embedding(100, 1000)\n",
              "  (linear): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (lm_head): Linear(in_features=1000, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model.linear)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "_wNgl7ZpP6fP",
        "outputId": "eaee64c4-71fc-4366-9e0e-c5bf0ed4a008"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.nn.modules.linear.Linear"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.linear.Linear</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py</a>Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
              "\n",
              "This module supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.\n",
              "\n",
              "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision&lt;fp16_on_mi200&gt;` for backward.\n",
              "\n",
              "Args:\n",
              "    in_features: size of each input sample\n",
              "    out_features: size of each output sample\n",
              "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
              "        Default: ``True``\n",
              "\n",
              "Shape:\n",
              "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
              "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
              "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
              "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
              "\n",
              "Attributes:\n",
              "    weight: the learnable weights of the module of shape\n",
              "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
              "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
              "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
              "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
              "            If :attr:`bias` is ``True``, the values are initialized from\n",
              "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
              "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
              "\n",
              "Examples::\n",
              "\n",
              "    &gt;&gt;&gt; m = nn.Linear(20, 30)\n",
              "    &gt;&gt;&gt; input = torch.randn(128, 20)\n",
              "    &gt;&gt;&gt; output = m(input)\n",
              "    &gt;&gt;&gt; print(output.size())\n",
              "    torch.Size([128, 30])</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 50);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_inputs = torch.LongTensor([[0,1,2,3,4,5,6,7]])\n",
        "dummy_inputs.shape,model(dummy_inputs).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-rt3vCzP8Ua",
        "outputId": "d644af35-4db5-4798-bc6c-0231417e3774"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 8]), torch.Size([1, 8, 100]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Embedding(10, 5)(dummy_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojv2IIDfQ30k",
        "outputId": "37a56738-f4d1-4c37-ba04-33cdaf8b22ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1791, -0.9010, -1.0093,  1.4889, -0.2674],\n",
              "         [-0.0299,  0.6660,  1.1523, -0.6777, -0.7593],\n",
              "         [-0.3459,  0.1020,  0.1680, -0.2007,  1.1532],\n",
              "         [ 2.0739, -1.6505,  0.0533,  0.2430,  1.0685],\n",
              "         [ 0.1663, -0.4189, -1.0534,  1.0814, -1.3158],\n",
              "         [ 1.3785, -1.0215, -0.3025,  0.5205,  0.6224],\n",
              "         [-0.3105,  0.9892, -0.1884,  1.8926,  0.4840],\n",
              "         [-1.4149,  0.5178, -0.6076,  0.6289,  0.4407]]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inspect.getsource(nn.Embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0bZ_R-dHQFXh",
        "outputId": "070e28a3-4653-4e7f-d938-406602c0fef9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class Embedding(Module):\n",
            "    r\"\"\"A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
            "\n",
            "    This module is often used to store word embeddings and retrieve them using indices.\n",
            "    The input to the module is a list of indices, and the output is the corresponding\n",
            "    word embeddings.\n",
            "\n",
            "    Args:\n",
            "        num_embeddings (int): size of the dictionary of embeddings\n",
            "        embedding_dim (int): the size of each embedding vector\n",
            "        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
            "                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
            "                                     i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n",
            "                                     the embedding vector at :attr:`padding_idx` will default to all zeros,\n",
            "                                     but can be updated to another value to be used as the padding vector.\n",
            "        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
            "                                    is renormalized to have norm :attr:`max_norm`.\n",
            "        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
            "        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n",
            "                                                the words in the mini-batch. Default ``False``.\n",
            "        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n",
            "                                 See Notes for more details regarding sparse gradients.\n",
            "\n",
            "    Attributes:\n",
            "        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
            "                         initialized from :math:`\\mathcal{N}(0, 1)`\n",
            "\n",
            "    Shape:\n",
            "        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
            "        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n",
            "\n",
            "    .. note::\n",
            "        Keep in mind that only a limited number of optimizers support\n",
            "        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n",
            "        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n",
            "\n",
            "    .. note::\n",
            "        When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n",
            "        :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n",
            "        modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n",
            "        calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n",
            "        :attr:`max_norm` is not ``None``. For example::\n",
            "\n",
            "            n, d, m = 3, 5, 7\n",
            "            embedding = nn.Embedding(n, d, max_norm=True)\n",
            "            W = torch.randn((m, d), requires_grad=True)\n",
            "            idx = torch.tensor([1, 2])\n",
            "            a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
            "            b = embedding(idx) @ W.t()  # modifies weight in-place\n",
            "            out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
            "            loss = out.sigmoid().prod()\n",
            "            loss.backward()\n",
            "\n",
            "    Examples::\n",
            "\n",
            "        >>> # an Embedding module containing 10 tensors of size 3\n",
            "        >>> embedding = nn.Embedding(10, 3)\n",
            "        >>> # a batch of 2 samples of 4 indices each\n",
            "        >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
            "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            "        >>> embedding(input)\n",
            "        tensor([[[-0.0251, -1.6902,  0.7172],\n",
            "                 [-0.6431,  0.0748,  0.6969],\n",
            "                 [ 1.4970,  1.3448, -0.9685],\n",
            "                 [-0.3677, -2.7265, -0.1685]],\n",
            "\n",
            "                [[ 1.4970,  1.3448, -0.9685],\n",
            "                 [ 0.4362, -0.4004,  0.9400],\n",
            "                 [-0.6431,  0.0748,  0.6969],\n",
            "                 [ 0.9124, -2.3616,  1.1151]]])\n",
            "\n",
            "\n",
            "        >>> # example with padding_idx\n",
            "        >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
            "        >>> input = torch.LongTensor([[0, 2, 0, 5]])\n",
            "        >>> embedding(input)\n",
            "        tensor([[[ 0.0000,  0.0000,  0.0000],\n",
            "                 [ 0.1535, -2.0309,  0.9315],\n",
            "                 [ 0.0000,  0.0000,  0.0000],\n",
            "                 [-0.1655,  0.9897,  0.0635]]])\n",
            "\n",
            "        >>> # example of changing `pad` vector\n",
            "        >>> padding_idx = 0\n",
            "        >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n",
            "        >>> embedding.weight\n",
            "        Parameter containing:\n",
            "        tensor([[ 0.0000,  0.0000,  0.0000],\n",
            "                [-0.7895, -0.7089, -0.0364],\n",
            "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
            "        >>> with torch.no_grad():\n",
            "        ...     embedding.weight[padding_idx] = torch.ones(3)\n",
            "        >>> embedding.weight\n",
            "        Parameter containing:\n",
            "        tensor([[ 1.0000,  1.0000,  1.0000],\n",
            "                [-0.7895, -0.7089, -0.0364],\n",
            "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
            "    \"\"\"\n",
            "\n",
            "    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n",
            "                     'norm_type', 'scale_grad_by_freq', 'sparse']\n",
            "\n",
            "    num_embeddings: int\n",
            "    embedding_dim: int\n",
            "    padding_idx: Optional[int]\n",
            "    max_norm: Optional[float]\n",
            "    norm_type: float\n",
            "    scale_grad_by_freq: bool\n",
            "    weight: Tensor\n",
            "    freeze: bool\n",
            "    sparse: bool\n",
            "\n",
            "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,\n",
            "                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,\n",
            "                 sparse: bool = False, _weight: Optional[Tensor] = None, _freeze: bool = False,\n",
            "                 device=None, dtype=None) -> None:\n",
            "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
            "        super().__init__()\n",
            "        self.num_embeddings = num_embeddings\n",
            "        self.embedding_dim = embedding_dim\n",
            "        if padding_idx is not None:\n",
            "            if padding_idx > 0:\n",
            "                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
            "            elif padding_idx < 0:\n",
            "                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'\n",
            "                padding_idx = self.num_embeddings + padding_idx\n",
            "        self.padding_idx = padding_idx\n",
            "        self.max_norm = max_norm\n",
            "        self.norm_type = norm_type\n",
            "        self.scale_grad_by_freq = scale_grad_by_freq\n",
            "        if _weight is None:\n",
            "            self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),\n",
            "                                    requires_grad=not _freeze)\n",
            "            self.reset_parameters()\n",
            "        else:\n",
            "            assert list(_weight.shape) == [num_embeddings, embedding_dim], \\\n",
            "                'Shape of weight does not match num_embeddings and embedding_dim'\n",
            "            self.weight = Parameter(_weight, requires_grad=not _freeze)\n",
            "\n",
            "        self.sparse = sparse\n",
            "\n",
            "    def reset_parameters(self) -> None:\n",
            "        init.normal_(self.weight)\n",
            "        self._fill_padding_idx_with_zero()\n",
            "\n",
            "    def _fill_padding_idx_with_zero(self) -> None:\n",
            "        if self.padding_idx is not None:\n",
            "            with torch.no_grad():\n",
            "                self.weight[self.padding_idx].fill_(0)\n",
            "\n",
            "    def forward(self, input: Tensor) -> Tensor:\n",
            "        return F.embedding(\n",
            "            input, self.weight, self.padding_idx, self.max_norm,\n",
            "            self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\n",
            "    def extra_repr(self) -> str:\n",
            "        s = '{num_embeddings}, {embedding_dim}'\n",
            "        if self.padding_idx is not None:\n",
            "            s += ', padding_idx={padding_idx}'\n",
            "        if self.max_norm is not None:\n",
            "            s += ', max_norm={max_norm}'\n",
            "        if self.norm_type != 2:\n",
            "            s += ', norm_type={norm_type}'\n",
            "        if self.scale_grad_by_freq is not False:\n",
            "            s += ', scale_grad_by_freq={scale_grad_by_freq}'\n",
            "        if self.sparse is not False:\n",
            "            s += ', sparse=True'\n",
            "        return s.format(**self.__dict__)\n",
            "\n",
            "    @classmethod\n",
            "    def from_pretrained(cls, embeddings, freeze=True, padding_idx=None,\n",
            "                        max_norm=None, norm_type=2., scale_grad_by_freq=False,\n",
            "                        sparse=False):\n",
            "        r\"\"\"Create Embedding instance from given 2-dimensional FloatTensor.\n",
            "\n",
            "        Args:\n",
            "            embeddings (Tensor): FloatTensor containing weights for the Embedding.\n",
            "                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.\n",
            "            freeze (bool, optional): If ``True``, the tensor does not get updated in the learning process.\n",
            "                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``\n",
            "            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
            "                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
            "                                         i.e. it remains as a fixed \"pad\".\n",
            "            max_norm (float, optional): See module initialization documentation.\n",
            "            norm_type (float, optional): See module initialization documentation. Default ``2``.\n",
            "            scale_grad_by_freq (bool, optional): See module initialization documentation. Default ``False``.\n",
            "            sparse (bool, optional): See module initialization documentation.\n",
            "\n",
            "        Examples::\n",
            "\n",
            "            >>> # FloatTensor containing pretrained weights\n",
            "            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
            "            >>> embedding = nn.Embedding.from_pretrained(weight)\n",
            "            >>> # Get embeddings for index 1\n",
            "            >>> input = torch.LongTensor([1])\n",
            "            >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            "            >>> embedding(input)\n",
            "            tensor([[ 4.0000,  5.1000,  6.3000]])\n",
            "        \"\"\"\n",
            "        assert embeddings.dim() == 2, \\\n",
            "            'Embeddings parameter is expected to be 2-dimensional'\n",
            "        rows, cols = embeddings.shape\n",
            "        embedding = cls(\n",
            "            num_embeddings=rows,\n",
            "            embedding_dim=cols,\n",
            "            _weight=embeddings,\n",
            "            _freeze=freeze,\n",
            "            padding_idx=padding_idx,\n",
            "            max_norm=max_norm,\n",
            "            norm_type=norm_type,\n",
            "            scale_grad_by_freq=scale_grad_by_freq,\n",
            "            sparse=sparse)\n",
            "        return embedding\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8Xxx1GQQPzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}